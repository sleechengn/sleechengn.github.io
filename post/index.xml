<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 人工智能艺术</title>
    <link>http://www.art4ai.com/post/</link>
    <description>Recent content in Posts on 人工智能艺术</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh_CN</language>
    <copyright>All rights reserved - 2017</copyright>
    <lastBuildDate>Sun, 29 Apr 2018 18:54:40 +0800</lastBuildDate>
    
	<atom:link href="http://www.art4ai.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deepleaning4j简单介绍安装</title>
      <link>http://www.art4ai.com/post/deeplearning4j_stp/</link>
      <pubDate>Sun, 29 Apr 2018 18:54:40 +0800</pubDate>
      
      <guid>http://www.art4ai.com/post/deeplearning4j_stp/</guid>
      <description>简介 deeplearning4j是一个基于jvm的机器学习框架，主要应用于商业环境下的机器学习应用场景。为什么这样説呢？因为它提供了商业化环境下运行的支撑体系，比如GPU、Spark等的支持，一般来説对于理论验证来説是不需要spark这些的，所以它是为商业应用打造的，它的主要维护机构来自于一家美国的初创公司：skymind.
主页地址：http://www.deeplearning4j.org
中文说明：http://www.deeplearning4j.org/cn
github:https://github.com/deeplearning4j
安装前准备： * apache maven * java sdk1.8 * eclipse/idea * git
以上这些是你需要安装的
maven主要用于管理项目所依赖的库，用于管理java的jar仓库，它能让你方便的引入一个软件栈。
java不必説了，它的下载地址在oracle，请自行前往下载，注意版本。
eclipse/idea是开发工具，也就是ide，一个好的开发工具会大大增加效率，因为本人使用的是eclipse和idea，java还有很多开发工具，这里就不一一说明了。但是有一点要注意，选用的ide必须支持maven管理依赖包，不然手动配置会很麻烦。
git是一个代码维护工具，类似的还有svn，主要用于代码管理
安装java 请在oracle下载java sdk，注意下载1.8版本，虽然其它版本也许支持，但是不同的版本有一定的区别，为了少走弯路用1.8.
下载deeplearning4j演示 github地址：https://github.com/deeplearning4j/dl4j-examples.git
将代码下载到本地。
下面以idea为例讲一下如何导入运行
配置git 把git的路径配置好，不然git可能不能正常工作，后面不没有办法了。
配置maven maven home 一定要配置好,就是你maven所有的目录路径
还有就是User setting，选override，指定maven目录下的conf/settings.xml 因为这样可以使用自己定义的镜像，用阿里云挺快，所以可以使用阿里云的仓库。
阿里云：
&amp;lt;mirror&amp;gt; &amp;lt;id&amp;gt;alimaven&amp;lt;/id&amp;gt; &amp;lt;name&amp;gt;aliyun maven&amp;lt;/name&amp;gt; &amp;lt;url&amp;gt;http://maven.aliyun.com/nexus/content/groups/public/&amp;lt;/url&amp;gt; &amp;lt;mirrorOf&amp;gt;central&amp;lt;/mirrorOf&amp;gt; &amp;lt;/mirror&amp;gt;  这个设置可以加速你的下载，因为你在国内访问中央仓库很慢，把这个配置就复制到settings.xml里的mirrors里面就行
开始导入
开始下载jar包了，这个要等一会儿，只要没中断就等。
在idea里配置java8就不説了，这些问题做为常识自己解决。 然后就可以运行了，下面讲讲几个特别坑的环境问题。
电脑只有cpu没有GPU怎么办 如果你电脑没有nvidia显卡或者还没有安装cuda工具以及cudnn，那么请在主pom.xml里设置
&amp;lt;nd4j.backend&amp;gt;nd4j-native-platform&amp;lt;/nd4j.backend&amp;gt;  如果你的电脑里有显示卡并安装了cuda工具，则按对应的版本修改
&amp;lt;nd4j.backend&amp;gt;nd4j-cuda-9.1-platform&amp;lt;/nd4j.backend&amp;gt;  假定你用的cpu，还有一个坑，用cpu也启动不了。你要把对应pom.xml 里的下列依赖去掉
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.nd4j&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;nd4j-cuda-8.0-platform&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${nd4j.version}&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt;  这些不同版本都要去掉，这样再启动，如果还有一个错误，就是没有mkl,这是最后一个坑。 请按图中设置启动参数
-Djava.library.path  加上这个参数就可以。</description>
    </item>
    
    <item>
      <title>神经网络之BP</title>
      <link>http://www.art4ai.com/post/bp_se/</link>
      <pubDate>Sun, 29 Apr 2018 15:56:10 +0800</pubDate>
      
      <guid>http://www.art4ai.com/post/bp_se/</guid>
      <description>１、前置知识 阅读这篇文章时，假定你了解以下知识：
1、神经网络输入输出的计算 2、损失函数 3、导数 　我觉得在这之前，你必须要把上面三个概念弄清楚，因为接下来的内容是以上面三个知识为基础的讲解，所以我们还是来复习一下上面三个知识点，如果你有不清楚的，可以看我另一篇文章或是其它博文。
　神经网络不说了，如果你不清楚定义，可以点上面的链接看我的另一篇博文，当然你可以查资料，但是在读这篇博文的同时请你务必要搞清楚，下面还是说一下损失函数和导数吧。
　损失函数$$loss$$，我们回顾一下定义，其实就是网络的实际输出和你想要的输出之间的差距，是一个欧氏距离，这个你学过 几何应该知道怎么计算，也就是平方和，我们有一个专业名词叫L2范数。另外还有一个重要的概念就是，可以通过调整权重偏执等参数来使损失函数变小，我们现在所说的损失函数若没有说明，默认是指网络输出的一个N维的点和希望输出的目标点之间的距离。
2、导数 　我们看看导数吧，这个很重要，因为导数是关键，它是让网络往目标更新的关键，我不像其它博文那样讲，我用最直观的来讲，就从导数本身开始。我们先来看看导数的定义，从定义出发总是没有错的，对于一个曲线来说，导数是什么？
　我们以二维的曲线为例吧，对于一个连续的曲线来说，我们可以用一个函数来表示，假设我们令这个函数关系是：
$$ y=f(x)$$
　我们来看看导数的定义，对于某一个点(x0,y0)来说，在这个点的导数定义是：
$$ \frac {df(x_0)}{dx0}=\lim{dx\to0}\frac{f(x_0+d x) - f(x_0)}{d x} $$
这个一定要知道导数的定义 $$\text{在}x_0\text{处的导数定义}$$
　最原始的导数定义，如果这个不会，你就去翻上学时的内容，对于我们来说，不难看出，**导数就是该点的斜率； $$ \text{如果我们要通过改变} x_0 \text{的值让}f(x0) \text{变小，我们必须把}x0\text{往导数反方向移动}$$ 根据定义，这个动作可以写这样的赋值： $$x_0=x_0-step*f&amp;rsquo;(x_0)$$
　参数说明
　step代表移动的步长系数，一般很小，不能移太大，因为移太大可能一步移过了最低点
　f&amp;rsquo;(x0)代表在x0处的导数
　如果$$f&amp;rsquo;(x_0)&amp;gt;0$$，则往左移动$$step*|f&amp;rsquo;(x_0)|$$，右边竖线是导数的绝对值
　如果$$f&amp;rsquo;(x_0)&amp;lt;0$$，则往右移动$$step*|f&amp;rsquo;(x_0)|$$，右边竖线是导数的绝对值
　经过不断的计算导数然后不断的这样移动后，我们的f(x0)就不断的变小，直到让它成0，无法再移动，这样f(x0)就达到了一个极小值。
　我们有了一个让f(x0)缩小的方法了，我们再来看看我们接下来重要的内容，我知道上面的可能对有人来说很基础，对有人来说却很困难，所以这个一定要确保能理解上面的方法，才能看下面的内容。
　我们来看看，在神经网络里，从通过类似上微小移动每个参数来改变$$loss$$。现在我们要讲方法，所以请务必把上面的东西看完并确保能完全理解，因为那是最基础的，所有后面的知识都基于上面的知识。
　我们找到了一个在连续曲线上找到较小值的方法，就是调整某个参数，按它的导数反向微小移动的方式。
　我们知道神经网络有权重还有偏执，我们这里用符号来表示，权重使用$$w_1,w_2,w_3,w_4,w_5,w_n$$,,,等来表示，偏执用$$b_1,b_2,b_3,b_4,b_5,b_n$$,,表示，我们这些都是网络中的参数，我们最终的目的是要调整这些参数的大小，让$$ loss $$最小。
　我们知道神经网络是一个从输入层输入一个数据，然后通过计算流程，最终输出一数据，再与目标数据计算出一个loss，我们一定要清楚这个，特别是loss不是输出数据，而是计算结果与期望目标的欧氏距离。那么，对于一个确定的固定的输入神经网络，LOSS的大小取决于权重和偏执，调整其中一个权重，就意味着LOSS就会变化，那么同样的，我们根据导数的定义：调整其中一个权重$$w_n$$使权重改变一个很小的$$dw$$，调整以后$$loss$$就会改变，对应的$$loss$$有一个小的改变量，我们简写为$$dloss$$，那么根据导数的定义，我们可以得出：对于此刻的状态，损失对某个权重的导数可以表示为：
$$\frac{dloss}{dw}$$
　这个解释一下，就是説移动一个参数后LOSS也会移动，两个的比值就是一个导数，也可当成斜率。
　这就是此刻状态loss对wn的导数，由于wn可以是任意一个权重，所以我们称以上式子为此刻loss对wn的偏导数 $$ \frac {\partial loss}{\partial w_n}$$ 这个是函数里的定义，所以不要问为什么这么取名字，原因是有多个自变量，每个变量的导数就是偏导数。同理，我们可以求出loss对任意的某个权重或某个偏执的偏导数，因为你只要把这个参数移动改一个微小量，对比loss的变化量就行。这样我们从理论上可以计算loss对任意参数的偏导数了。</description>
    </item>
    
    <item>
      <title>神经网络入门</title>
      <link>http://www.art4ai.com/post/bp_first/</link>
      <pubDate>Sat, 28 Apr 2018 23:48:13 +0800</pubDate>
      
      <guid>http://www.art4ai.com/post/bp_first/</guid>
      <description>1、什么是神经网络，它是如何计算的。 　首先来看神经网络长什么样子：
神经网络
　图中表示的是一个输入层是4，中间层为3，输出层2的神经网络，每个圈表示一个神经元，比如,,等都是神经元。上图是一个的神经网络。实际网络可以有任意的输入，任意的中间层，任意的输出数量。比如可以是这样任意的层数。
$$ 1000*100*10*5 $$
图中每一条连接的线叫做权重，比如 $$ x1-&amp;gt;h1$$ 这条线。
我们表示:
$$ W{x{1}h_{1}}$$
　了解了这些以后，我们来看一下神经网络的计算
　神经网络是从输入层计算，到中间层，一层一层计算到输出，首先对输入层赋值，然后计算第二层，最后计算y1,y2输出。
h1的计算方法是：
假定我们用$$ W{x{i}h_{j}}$$代表连接$$ x_1 $$和 $$ h_1 $$的权重，则h1的计算可以写为：
$$ x1 * w{x_1h_1} + x2 * w{x_2h_1} + x3 * w{x_3h_1} + x4 * w{x_4h_1}$$
本层神经元的输入定义：
$$ hj=\sum{i=1}^{features}xi*w{x_ih_j} + bias_j $$ 这里说明一下，输入层神经元下标i,输出层下标j,这样就定入了神经元的净输入，根据上面的网络图再结合公式理解。其实就是一个加权求和的过程，注意bias代表神经元的偏执。
神经元的净输入
bias-偏执（表示本层每个神经元的偏执，注意，每个本层神经元都有一个偏执，就是一个数字） xi代表输入层第i个神经元的值，wij代表连接输入层第i个神经元，本层第j个神经元的权重。
　每个神经元的输入，就是与这个神经元相连的上一层神经元乘以权重然后再神经元处相加，后面再加上偏执，这就是神经元的输入。
每个神经元的输出等于神经元输入应用一个激活函数，比如SIGMOID，公式如下：
$$ output = sigmoid(input) $$
　参数：output-输出 input-神经元输入 sigmoid-一个S形函数，原型： $$ y= \frac{1}{1+e^{-x}}$$</description>
    </item>
    
    <item>
      <title>线性代数知识</title>
      <link>http://www.art4ai.com/post/linear_x/</link>
      <pubDate>Sat, 28 Apr 2018 23:48:13 +0800</pubDate>
      
      <guid>http://www.art4ai.com/post/linear_x/</guid>
      <description> 线代基础知识 对于机器学习来説，线性代数方便我们更好的理解，我们通过线性代数的方法更好的表示机器学习，这里线性代数指的是一些基本的线性代数内容，并不是高深的所有的内容。
1、行列式 本章主要介绍n阶行列式的定义、性质、计算方法，此外还要讲解我n阶行列式计算n元线性方程组的克拉默法则。
二阶与三阶行列式 二元线性方程组与二阶行列式：
$$ \left { \begin{array}{c} a_{11}x1+a{12}x_2=b1 \ a{21}x1+a{22}x_2=b_2 \end{array} \right. $$
$$ \text {为了消去未知数} x2 \text {,以} a{22} \text{与}a_{12}\text{分别乘上列两方程的两端,然后两个方程相减,得} $$
$$ (a{11}a{22} - a{12}a{21})x_1=b1a{22} - a_{12}b_2 $$
$$ \text {类似地，消去} x_2 \text {，得} $$
$$ (a{12}a{21} - a{11}a{22})x_2 = b1a{21} - a_{11}b_2 $$
 </description>
    </item>
    
  </channel>
</rss>