<!DOCTYPE html>
<html lang="en-EN">

<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">

<meta name="description" content="更新器 更新器一个神经网络进行调节的策略，我们普通的SGD随机梯度下降会有很多局限，为了解决这些可以优化的问题引入多种策略，这些策略能加速神经网络的训练。
我们也称更新器叫优化器。
常用的更新器
 1、SGD 随机梯度下降
 2、MOMENT 动量
 3、Nesterov 牛顿下降
 4、Adamgrad
 5、Adamdelta
 6、RMSProp
 7、Adam
  SGD 这个是我们最常用的算法了，随机梯度下降，我们先来看一下梯度下降。 普通的梯度下降是指我们一次性将所有的数据给网络，由网络反向计算loss对每个参数的偏导数，这个偏数对应的是所有的数据，所以这个梯度非常的准确。但是这样有一个问题就是当数据量大的时候不管是计算还是存储资源都会占用很大，这样使得我们很难训练神经网络参数，普通的GD对于小网络来说可以使用，对于大网络来说就不要去使用了，因为它的计算量非常大。
SDG的更新方法：
$$ \Delta_{\theta_t} = - \eta g_t $$
Moment 动量是一常见的更新策略，在动量的帮助下，可以加速神经网络的收敛，可以有效的节省收敛时间，因为动量是累积的，所以可以很快的方式达到收敛。 公式：
参数说明：
$$ \text{参数：} \mu \text{ 衰减系数} $$
$$ \text{参数：} v_{t-1} \text{ 上一次的速度} $$
$$ \text{参数：} g_t \text{ 梯度} $$
$$ \text{参数：} \eta \text{ 学习率} $$
注意，衰减系数不能大于1，只能小于1，不能小于0。
Nesterov 这个牛顿法则是比运量更进一步，它首先更新参数，然后再进行动量计算，这样以来我们收剑速度可能更快，而且可以收反馈。
这种先更新，再来求梯度，然后再运量累积的做法实际上是加速二阶动量的方式，这种方法比动量在某些网络中表现更好。">

<meta property="og:title" content="更新器简介" />
<meta property="og:description" content="更新器 更新器一个神经网络进行调节的策略，我们普通的SGD随机梯度下降会有很多局限，为了解决这些可以优化的问题引入多种策略，这些策略能加速神经网络的训练。
我们也称更新器叫优化器。
常用的更新器
 1、SGD 随机梯度下降
 2、MOMENT 动量
 3、Nesterov 牛顿下降
 4、Adamgrad
 5、Adamdelta
 6、RMSProp
 7、Adam
  SGD 这个是我们最常用的算法了，随机梯度下降，我们先来看一下梯度下降。 普通的梯度下降是指我们一次性将所有的数据给网络，由网络反向计算loss对每个参数的偏导数，这个偏数对应的是所有的数据，所以这个梯度非常的准确。但是这样有一个问题就是当数据量大的时候不管是计算还是存储资源都会占用很大，这样使得我们很难训练神经网络参数，普通的GD对于小网络来说可以使用，对于大网络来说就不要去使用了，因为它的计算量非常大。
SDG的更新方法：
$$ \Delta_{\theta_t} = - \eta g_t $$
Moment 动量是一常见的更新策略，在动量的帮助下，可以加速神经网络的收敛，可以有效的节省收敛时间，因为动量是累积的，所以可以很快的方式达到收敛。 公式：
参数说明：
$$ \text{参数：} \mu \text{ 衰减系数} $$
$$ \text{参数：} v_{t-1} \text{ 上一次的速度} $$
$$ \text{参数：} g_t \text{ 梯度} $$
$$ \text{参数：} \eta \text{ 学习率} $$
注意，衰减系数不能大于1，只能小于1，不能小于0。
Nesterov 这个牛顿法则是比运量更进一步，它首先更新参数，然后再进行动量计算，这样以来我们收剑速度可能更快，而且可以收反馈。
这种先更新，再来求梯度，然后再运量累积的做法实际上是加速二阶动量的方式，这种方法比动量在某些网络中表现更好。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://www.ml4ai.com/2018/04/28/%E6%9B%B4%E6%96%B0%E5%99%A8%E7%AE%80%E4%BB%8B/" />



<meta property="article:published_time" content="2018-04-28T23:48:13&#43;08:00"/>

<meta property="article:modified_time" content="2018-04-28T23:48:13&#43;08:00"/>












<title>


     更新器简介 

</title>
<link rel="canonical" href="http://www.ml4ai.com/2018/04/28/%E6%9B%B4%E6%96%B0%E5%99%A8%E7%AE%80%E4%BB%8B/">










<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700|Ubuntu+Mono:400,400i,700,700i|Raleway:500">



    <link rel="stylesheet" href="/css/reset.css">
    <link rel="stylesheet" href="/css/pygments.css">
    <link rel="stylesheet" href="/css/main.css">
    




<link rel="shortcut icon"

    href="/img/favicon.ico"

>








</head>


<body lang="">

<section class="header">
    <div class="container">
        <div class="content">
            
            <a href="/"><div class="name"></div></a>
            
            <nav>
                <ul>
                    
                </ul>
            </nav>
        </div>
    </div>
</section>

<section class="icons">
    <div class="container">
        <div class="content">
        
            <a href="enten/hugo-boilerplate" target="_blank" rel="noopener"><img class="icon" src="/img/github.svg" alt="github" /></a>
        

        

	

        

        

        

        

        

        

        

        

        

        
        </div>
    </div>
</section>

<section class="main">
    <div class="container">
        <div class="content">
            <div class="page-heading">

    更新器简介

</div>

            <div class="markdown">
                

<h1 id="更新器">更新器</h1>

<p>更新器一个神经网络进行调节的策略，我们普通的SGD随机梯度下降会有很多局限，为了解决这些可以优化的问题引入多种策略，这些策略能加速神经网络的训练。</p>

<p>我们也称更新器叫优化器。</p>

<p><strong>常用的更新器</strong></p>

<ul>
<li><p>1、SGD 随机梯度下降</p></li>

<li><p>2、MOMENT 动量</p></li>

<li><p>3、Nesterov 牛顿下降</p></li>

<li><p>4、Adamgrad</p></li>

<li><p>5、Adamdelta</p></li>

<li><p>6、RMSProp</p></li>

<li><p>7、Adam</p></li>
</ul>

<h2 id="sgd">SGD</h2>

<p>这个是我们最常用的算法了，随机梯度下降，我们先来看一下梯度下降。
普通的梯度下降是指我们一次性将所有的数据给网络，由网络反向计算loss对每个参数的偏导数，这个偏数对应的是所有的数据，所以这个梯度非常的准确。但是这样有一个问题就是当数据量大的时候不管是计算还是存储资源都会占用很大，这样使得我们很难训练神经网络参数，普通的GD对于小网络来说可以使用，对于大网络来说就不要去使用了，因为它的计算量非常大。</p>

<p>SDG的更新方法：</p>

<p>$$ \Delta_{\theta_t} = - \eta g_t $$</p>

<h2 id="moment">Moment</h2>

<p>动量是一常见的更新策略，在动量的帮助下，可以加速神经网络的收敛，可以有效的节省收敛时间，因为动量是累积的，所以可以很快的方式达到收敛。
公式：</p>

<p><img src="http://www.ml4ai.com/images/moment-update-x.png" alt="image" /></p>

<p>参数说明：</p>

<p>$$ \text{参数：} \mu \text{  衰减系数} $$</p>

<p>$$ \text{参数：} v_{t-1} \text{  上一次的速度} $$</p>

<p>$$ \text{参数：} g_t \text{  梯度} $$</p>

<p>$$ \text{参数：} \eta \text{  学习率} $$</p>

<p>注意，衰减系数不能大于1，只能小于1，不能小于0。</p>

<h2 id="nesterov">Nesterov</h2>

<p>这个牛顿法则是比运量更进一步，它首先更新参数，然后再进行动量计算，这样以来我们收剑速度可能更快，而且可以收反馈。</p>

<p><img src="http://www.ml4ai.com/images/nesterov-update_x.png" alt="image" /></p>

<p>这种先更新，再来求梯度，然后再运量累积的做法实际上是加速二阶动量的方式，这种方法比动量在某些网络中表现更好。</p>

            </div>
        </div>
    </div>
</section>








</body>
</html>

