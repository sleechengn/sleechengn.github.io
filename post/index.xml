<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 深度学习与人工智能</title>
    <link>http://www.ml4ai.com/post/</link>
    <description>Recent content in Posts on 深度学习与人工智能</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh_CN</language>
    <copyright>All rights reserved - 2017</copyright>
    <lastBuildDate>Sun, 29 Apr 2018 18:54:40 +0800</lastBuildDate>
    
	<atom:link href="http://www.ml4ai.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deepleaning4j简单介绍安装</title>
      <link>http://www.ml4ai.com/post/deeplearning4j_stp/</link>
      <pubDate>Sun, 29 Apr 2018 18:54:40 +0800</pubDate>
      
      <guid>http://www.ml4ai.com/post/deeplearning4j_stp/</guid>
      <description>简介 deeplearning4j是一个基于jvm的机器学习框架，主要应用于商业环境下的机器学习应用场景。为什么这样説呢？因为它提供了商业化环境下运行的支撑体系，比如GPU、Spark等的支持，一般来説对于理论验证来説是不需要spark这些的，所以它是为商业应用打造的，它的主要维护机构来自于一家美国的初创公司：skymind.
主页地址：http://www.deeplearning4j.org
中文说明：http://www.deeplearning4j.org/cn
github:https://github.com/deeplearning4j
安装前准备：
 apache maven
 java sdk1.8
 eclipse/idea
 git
  以上这些是你需要安装的
maven主要用于管理项目所依赖的库，用于管理java的jar仓库，它能让你方便的引入一个软件栈。
java不必説了，它的下载地址在oracle，请自行前往下载，注意版本。
eclipse/idea是开发工具，也就是ide，一个好的开发工具会大大增加效率，因为本人使用的是eclipse和idea，java还有很多开发工具，这里就不一一说明了。但是有一点要注意，选用的ide必须支持maven管理依赖包，不然手动配置会很麻烦。
git是一个代码维护工具，类似的还有svn，主要用于代码管理
安装java 请在oracle下载java sdk，注意下载1.8版本，虽然其它版本也许支持，但是不同的版本有一定的区别，为了少走弯路用1.8.
下载deeplearning4j演示 github地址：https://github.com/deeplearning4j/dl4j-examples.git
将代码下载到本地。
下面以idea为例讲一下如何导入运行
配置git 把git的路径配置好，不然git可能不能正常工作，后面不没有办法了。
配置maven maven home 一定要配置好,就是你maven所有的目录路径
还有就是User setting，选override，指定maven目录下的conf/settings.xml 因为这样可以使用自己定义的镜像，用阿里云挺快，所以可以使用阿里云的仓库。
阿里云：
&amp;lt;mirror&amp;gt; &amp;lt;id&amp;gt;alimaven&amp;lt;/id&amp;gt; &amp;lt;name&amp;gt;aliyun maven&amp;lt;/name&amp;gt; &amp;lt;url&amp;gt;http://maven.aliyun.com/nexus/content/groups/public/&amp;lt;/url&amp;gt; &amp;lt;mirrorOf&amp;gt;central&amp;lt;/mirrorOf&amp;gt; &amp;lt;/mirror&amp;gt;  这个设置可以加速你的下载，因为你在国内访问中央仓库很慢，把这个配置就复制到settings.xml里的mirrors里面就行
开始导入
开始下载jar包了，这个要等一会儿，只要没中断就等。
在idea里配置java8就不説了，这些问题做为常识自己解决。 然后就可以运行了，下面讲讲几个特别坑的环境问题。
电脑只有cpu没有GPU怎么办 如果你电脑没有nvidia显卡或者还没有安装cuda工具以及cudnn，那么请在主pom.xml里设置
&amp;lt;nd4j.backend&amp;gt;nd4j-native-platform&amp;lt;/nd4j.backend&amp;gt;  如果你的电脑里有显示卡并安装了cuda工具，则按对应的版本修改
&amp;lt;nd4j.backend&amp;gt;nd4j-cuda-9.1-platform&amp;lt;/nd4j.backend&amp;gt;  假定你用的cpu，还有一个坑，用cpu也启动不了。你要把对应pom.xml 里的下列依赖去掉
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.nd4j&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;nd4j-cuda-8.0-platform&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${nd4j.version}&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt;  这些不同版本都要去掉，这样再启动，如果还有一个错误，就是没有mkl,这是最后一个坑。 请按图中设置启动参数</description>
    </item>
    
    <item>
      <title>神经网络之BP</title>
      <link>http://www.ml4ai.com/post/bp_se/</link>
      <pubDate>Sun, 29 Apr 2018 15:56:10 +0800</pubDate>
      
      <guid>http://www.ml4ai.com/post/bp_se/</guid>
      <description>１、前置知识 阅读这篇文章时，假定你了解以下知识：
1、神经网络输入输出的计算 2、损失函数 3、导数 　我觉得在这之前，你必须要把上面三个概念弄清楚，因为接下来的内容是以上面三个知识为基础的讲解，所以我们还是来复习一下上面三个知识点，如果你有不清楚的，可以看我另一篇文章或是其它博文。
　神经网络不说了，如果你不清楚定义，可以点上面的链接看我的另一篇博文，当然你可以查资料，但是在读这篇博文的同时请你务必要搞清楚，下面还是说一下损失函数和导数吧。
　损失函数$$loss$$，我们回顾一下定义，其实就是网络的实际输出和你想要的输出之间的差距，是一个欧氏距离，这个你学过 几何应该知道怎么计算，也就是平方和，我们有一个专业名词叫L2范数。另外还有一个重要的概念就是，可以通过调整权重偏执等参数来使损失函数变小，我们现在所说的损失函数若没有说明，默认是指网络输出的一个N维的点和希望输出的目标点之间的距离。
2、导数 　我们看看导数吧，这个很重要，因为导数是关键，它是让网络往目标更新的关键，我不像其它博文那样讲，我用最直观的来讲，就从导数本身开始。我们先来看看导数的定义，从定义出发总是没有错的，对于一个曲线来说，导数是什么？
　我们以二维的曲线为例吧，对于一个连续的曲线来说，我们可以用一个函数来表示，假设我们令这个函数关系是：
$$ y=f(x)$$
　我们来看看导数的定义，对于某一个点(x0,y0)来说，在这个点的导数定义是：
$$ \text{在}x_0\text{处的导数定义} $$
　最原始的导数定义，如果这个不会，你就去翻上学时的内容，对于我们来说，不难看出，**导数就是该点的斜率； $$ \text{如果我们要通过改变} x_0 \text{的值让}f(x0) \text{变小，我们必须把}x0\text{往导数反方向移动}$$ 根据定义，这个动作可以写这样的赋值： $$x_0=x_0-step*f&amp;rsquo;(x_0)$$
　参数说明
　step代表移动的步长系数，一般很小，不能移太大，因为移太大可能一步移过了最低点
　f&amp;rsquo;(x0)代表在x0处的导数
　如果$$f&amp;rsquo;(x_0)&amp;gt;0$$，则往左移动$$step*|f&amp;rsquo;(x_0)|$$，右边竖线是导数的绝对值
　如果$$f&amp;rsquo;(x_0)&amp;lt;0$$，则往右移动$$step*|f&amp;rsquo;(x_0)|$$，右边竖线是导数的绝对值
　经过不断的计算导数然后不断的这样移动后，我们的f(x0)就不断的变小，直到让它成0，无法再移动，这样f(x0)就达到了一个极小值。
　我们有了一个让f(x0)缩小的方法了，我们再来看看我们接下来重要的内容，我知道上面的可能对有人来说很基础，对有人来说却很困难，所以这个一定要确保能理解上面的方法，才能看下面的内容。
　我们来看看，在神经网络里，从通过类似上微小移动每个参数来改变$$loss$$。现在我们要讲方法，所以请务必把上面的东西看完并确保能完全理解，因为那是最基础的，所有后面的知识都基于上面的知识。
　我们找到了一个在连续曲线上找到较小值的方法，就是调整某个参数，按它的导数反向微小移动的方式。
　我们知道神经网络有权重还有偏执，我们这里用符号来表示，权重使用$$w_1,w_2,w_3,w_4,w_5,w_n$$,,,等来表示，偏执用$$b_1,b_2,b_3,b_4,b_5,b_n$$,,表示，我们这些都是网络中的参数，我们最终的目的是要调整这些参数的大小，让$$ loss $$最小。
　我们知道神经网络是一个从输入层输入一个数据，然后通过计算流程，最终输出一数据，再与目标数据计算出一个loss，我们一定要清楚这个，特别是loss不是输出数据，而是计算结果与期望目标的欧氏距离。那么，对于一个确定的固定的输入神经网络，LOSS的大小取决于权重和偏执，调整其中一个权重，就意味着LOSS就会变化，那么同样的，我们根据导数的定义：调整其中一个权重$$w_n$$使权重改变一个很小的$$dw$$，调整以后$$loss$$就会改变，对应的$$loss$$有一个小的改变量，我们简写为$$dloss$$，那么根据导数的定义，我们可以得出：对于此刻的状态，损失对某个权重的导数可以表示为：
$$\frac{dloss}{dw}$$
　这个解释一下，就是説移动一个参数后LOSS也会移动，两个的比值就是一个导数，也可当成斜率。
　这就是此刻状态loss对wn的导数，由于wn可以是任意一个权重，所以我们称以上式子为此刻loss对wn的偏导数 $$ \frac {\partial loss}{\partial w_n}$$ 这个是函数里的定义，所以不要问为什么这么取名字，原因是有多个自变量，每个变量的导数就是偏导数。同理，我们可以求出loss对任意的某个权重或某个偏执的偏导数，因为你只要把这个参数移动改一个微小量，对比loss的变化量就行。这样我们从理论上可以计算loss对任意参数的偏导数了。
　还记得我们的曲线找最小值的方法吗，没错，你只要把参数值往偏导数反方向移动一个微小的值，也等于减小loss。</description>
    </item>
    
    <item>
      <title>Pytorch简介</title>
      <link>http://www.ml4ai.com/post/pytorch_rxeat/</link>
      <pubDate>Sat, 28 Apr 2018 23:48:13 +0800</pubDate>
      
      <guid>http://www.ml4ai.com/post/pytorch_rxeat/</guid>
      <description>Pytorch pytorch是一个张量前向运算、自动反向运算管理工具，对张量运算有很好的支持。
安装方法 我们先打开网站：http://pytorch.org 选择你的环境，比如linux，pip，cuda9。 完成以后生成命令，你把命令拷贝在你的cmd里执行即安装。
简单说明 pytorch是一个深度学习计算图构建工具，它相对于tensorflow来说更灵活，为什么这样说呢，是因为它在构建计算图的时候是实时的，它记录变量的生命周期，即变量从什么操作得到，操作类型等。这非常重要，这样在进行自动微分的时候就能通过这种关系来计算，而tensorflow本身只是一张计算图，图的东西不可以更新，其实就是预先设计一个计算流程，然后再计算，这样导致了在计算过程需要改变计算图的时候是不能支持的。 一般来说，研究人员选择pytorch验证，然后使用tensorflow打包模型服务。
资源 pytorch文档：http://pytorch-cn.readthedocs.io</description>
    </item>
    
    <item>
      <title>tensorflow</title>
      <link>http://www.ml4ai.com/post/tensorflow/</link>
      <pubDate>Sat, 28 Apr 2018 23:48:13 +0800</pubDate>
      
      <guid>http://www.ml4ai.com/post/tensorflow/</guid>
      <description>Tensorflow &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Tensorflow是一个Google推出的张量图计算工具，它并不是python编写的，只是支持python来调用它，所以tensorflow的性能是在大部分深度学习框架中比较快的，因为它的图模块运行在c++编写的引用模块中，它可以使用gpu进行运算，速度非常快
安装方法
首先安装anaconda。 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;（这个直接下载对应版本安装就可以）。
安装numpy &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;安装命令：pip install numpy
安装tensorflow &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;命令：pip install tensorflow,当你是GPU支持时，pip install tensorflow-gpu来安装GPU版本，注意GPU版本要正常工作你必须安装cuda和cudnn工具，并且正确的设置，否则你的tensorflow可能不能正常运行。
简单说明 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Tensorflow是一个张量计算图工具，在tensorflow中，每个tensor都有一个op和op参与的依赖tensor，这样形成一个op图，op图是可以微分的，所以tensorflow优化器能够将所有的tensor的导数都求出来，优化器会通过导数来更新参数，进行优化。
资源 anaconda: https://www.anaconda.com/</description>
    </item>
    
    <item>
      <title>更新器简介</title>
      <link>http://www.ml4ai.com/post/optimizer/</link>
      <pubDate>Sat, 28 Apr 2018 23:48:13 +0800</pubDate>
      
      <guid>http://www.ml4ai.com/post/optimizer/</guid>
      <description>更新器 更新器一个神经网络进行调节的策略，我们普通的SGD随机梯度下降会有很多局限，为了解决这些可以优化的问题引入多种策略，这些策略能加速神经网络的训练。
我们也称更新器叫优化器。
常用的更新器
 1、SGD 随机梯度下降
 2、MOMENT 动量
 3、Nesterov 牛顿下降
 4、Adamgrad
 5、Adamdelta
 6、RMSProp
 7、Adam
  SGD 这个是我们最常用的算法了，随机梯度下降，我们先来看一下梯度下降。 普通的梯度下降是指我们一次性将所有的数据给网络，由网络反向计算loss对每个参数的偏导数，这个偏数对应的是所有的数据，所以这个梯度非常的准确。但是这样有一个问题就是当数据量大的时候不管是计算还是存储资源都会占用很大，这样使得我们很难训练神经网络参数，普通的GD对于小网络来说可以使用，对于大网络来说就不要去使用了，因为它的计算量非常大。
SDG的更新方法：
$$ \Delta_{\theta_t} = - \eta g_t $$
Moment 动量是一常见的更新策略，在动量的帮助下，可以加速神经网络的收敛，可以有效的节省收敛时间，因为动量是累积的，所以可以很快的方式达到收敛。 公式：
$$ vt=\mu v{t-1} + g_t $$
$$ \Delta_{\theta_t} = - \eta v_t $$
参数说明：
$$ \text{参数：} \mu \text{ 衰减系数} $$
$$ \text{参数：} v_{t-1} \text{ 上一次的速度} $$
$$ \text{参数：} g_t \text{ 梯度} $$</description>
    </item>
    
    <item>
      <title>神经网络入门</title>
      <link>http://www.ml4ai.com/post/bp_first/</link>
      <pubDate>Sat, 28 Apr 2018 23:48:13 +0800</pubDate>
      
      <guid>http://www.ml4ai.com/post/bp_first/</guid>
      <description>1、什么是神经网络，它是如何计算的。 　首先来看神经网络长什么样子：
神经网络
　图中表示的是一个输入层是4，中间层为3，输出层2的神经网络，每个圈表示一个神经元，比如,,等都是神经元。上图是一个的神经网络。实际网络可以有任意的输入，任意的中间层，任意的输出数量。比如可以是这样任意的层数。
$$ 1000*100*10*5 $$
图中每一条连接的线叫做权重，比如 $$ x1-&amp;gt;h1$$ 这条线。
我们表示:
$$ W_{x_1h_1} $$
　了解了这些以后，我们来看一下神经网络的计算
　神经网络是从输入层计算，到中间层，一层一层计算到输出，首先对输入层赋值，然后计算第二层，最后计算y1,y2输出。
h1的计算方法是：
假定我们用 $$ W_{x_ih_j} $$ 代表连接 $$ x_i $$ 和 $$ h_j $$ 的权重，则h1的计算可以写为：
本层神经元的输入定义：
这里说明一下，输入层神经元下标i,输出层下标j,这样就定入了神经元的净输入，根据上面的网络图再结合公式理解。其实就是一个加权求和的过程，注意bias代表神经元的偏执。
神经元的净输入
bias-偏执（表示本层每个神经元的偏执，注意，每个本层神经元都有一个偏执，就是一个数字） xi代表输入层第i个神经元的值，wij代表连接输入层第i个神经元，本层第j个神经元的权重。
　每个神经元的输入，就是与这个神经元相连的上一层神经元乘以权重然后再神经元处相加，后面再加上偏执，这就是神经元的输入。
每个神经元的输出等于神经元输入应用一个激活函数，比如SIGMOID，公式如下：
$$ output = sigmoid(input) $$
　参数：output-输出 input-神经元输入 sigmoid-一个S形函数，原型： $$ y= \frac{1}{1+e^{-x}}$$
神经网络就这样计算到最后一层，每条连线都会发生计算，最后到y1、y2计算出一个输出值。
2、神经网络用来干什么。 　神经网络就是这样的一个计算网络，人们可能问，这个有什么用，网络本身可以根据一个输入，产生一个输出。
　在我们上面学习的过程中，我们已经了解网络中有哪些东西可以影响输出值，不难想象，给定输入值，网络中还有权重和偏执，权重和偏执的值可以影响网络的输出。那么可以得出一个结论，调整每个权重和每个神经元的偏执可以改变网络输出。
　网络是一个计算流程图，假定我们输入数组x，结果输出数组y，X和Ｙ都是一个数组，我们知道一个向量或者一个坐标点也可以数组表示，这里我们把向量或坐标Ｘ和Ｙ的映射关系表示为函数：
$$ Y=net(X) $$
　X向量经过net网络，输出Y，这样我们就可以通过一个定量的方式来表示数据，X数组的数字个数称为X的维度，那么图中X的维数是4，Y的维数是2，那么net函数的功能就是输入一个4维的点，输出一个二维的点。假定一个网络输入是M维，输出是N维，那么这个网络的功能就是输入一个M维的点，输出一个N维的点。
　我们知道改变权重和偏执可以改变网络的输出，那么可以得出一个结论：通过调整权重和偏执参数，可以把网络塑造成不同的函数。</description>
    </item>
    
    <item>
      <title>线性代数知识</title>
      <link>http://www.ml4ai.com/post/linear_x/</link>
      <pubDate>Sat, 28 Apr 2018 23:48:13 +0800</pubDate>
      
      <guid>http://www.ml4ai.com/post/linear_x/</guid>
      <description>线代基础知识 对于机器学习来説，线性代数方便我们更好的理解，我们通过线性代数的方法更好的表示机器学习，这里线性代数指的是一些基本的线性代数内容，并不是高深的所有的内容。
1、行列式 本章主要介绍n阶行列式的定义、性质、计算方法，此外还要讲解我n阶行列式计算n元线性方程组的克拉默法则。
二阶与三阶行列式 二元线性方程组与二阶行列式：
$$ \left { \begin{array}{c} a_{11}x1+a{12}x_2=b1 \ a{21}x1+a{22}x_2=b_2 \end{array} \right. $$
$$ \text {为了消去未知数} x2 \text {,以} a{22} \text{与}a_{12}\text{分别乘上列两方程的两端,然后两个方程相减,得} $$
$$ (a{11}a{22} - a{12}a{21})x_1=b1a{22} - a_{12}b_2 $$
$$ \text {类似地，消去} x_2 \text {，得} $$
$$ (a{12}a{21} - a{11}a{22})x_2 = b1a{21} - a_{11}b_2 $$</description>
    </item>
    
  </channel>
</rss>