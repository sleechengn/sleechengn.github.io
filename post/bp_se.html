<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>深度学习与人工智能</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="keywords" content="">
    <meta name="generator" content="JBake">

    <!-- Le styles -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <link href="../css/asciidoctor.css" rel="stylesheet">
    <link href="../css/base.css" rel="stylesheet">
    <link href="../css/prettify.css" rel="stylesheet">

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->

    <!-- Fav and touch icons -->
    <!--<link rel="apple-touch-icon-precomposed" sizes="144x144" href="../assets/ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../assets/ico/apple-touch-icon-114-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../assets/ico/apple-touch-icon-72-precomposed.png">
    <link rel="apple-touch-icon-precomposed" href="../assets/ico/apple-touch-icon-57-precomposed.png">-->
    <link rel="shortcut icon" href="../favicon.ico">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  </head>
  <body onload="prettyPrint()">
    <div id="wrap">
	
	<!-- Fixed navbar -->
    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">ML4AI</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="../index.html">首页</a></li>
            <li><a href="../post/about.html">关于</a></li>
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown">推荐博文<b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li><a href="/post/bp_first.html">神经网络入门</a></li>
                <li><a href="/post/bp_se.html">反向传播算法</a></li>
                <li class="divider"></li>
                <li><a href="/post/deeplearning4j_stp.html">Deeplearning4j</a></li>
                <li><a href="/post/tensorflow.html">Tensorflow</a></li>
                <li><a href="/post/pytorch_introduce.html">Pytorch</a></li>
              </ul>
            </li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>
    <div class="container">	
	<div class="page-header">
		<h1>反向传播</h1>
	</div>

	<p><em>22 五月 2018</em></p>

	<p><h1>１、前置知识</h1> 
<p>阅读这篇文章时，假定你了解以下知识：</p> 
<h2>1、神经网络输入输出的计算</h2> 
<h2>2、损失函数</h2> 
<h2>3、导数</h2> 
<p>　　我觉得在这之前，你必须要把上面三个概念弄清楚，因为接下来的内容是以上面三个知识为基础的讲解，所以我们还是来复习一下上面三个知识点，如果你有不清楚的，可以看我另一篇<a href="/post/bp_first/">文章</a>或是其它博文。</p> 
<p>　　神经网络不说了，如果你不清楚定义，可以点上面的链接看我的另一篇博文，当然你可以查资料，但是在读这篇博文的同时请你务必要搞清楚，下面还是说一下损失函数和导数吧。</p> 
<p>　　损失函数$$loss$$，我们回顾一下定义，其实就是网络的实际输出和你想要的输出之间的差距，是一个欧氏距离，这个你学过 几何应该知道怎么计算，也就是平方和，我们有一个专业名词叫L2范数。另外还有一个重要的概念就是，可以通过调整权重偏执等参数来使损失函数变小，我们现在所说的损失函数若没有说明，默认是指网络输出的一个N维的点和希望输出的目标点之间的距离。</p> 
<h1>2、导数</h1> 
<p>　　我们看看导数吧，这个很重要，因为导数是关键，它是让网络往目标更新的关键，我不像其它博文那样讲，我用最直观的来讲，就从导数本身开始。我们先来看看导数的定义，从定义出发总是没有错的，对于一个曲线来说，导数是什么？</p> 
<p>　　我们以二维的曲线为例吧，对于一个连续的曲线来说，我们可以用一个函数来表示，假设我们令这个函数关系是：</p> 
<p>$$ y=f(x)$$</p> 
<p>　　我们来看看导数的定义，对于某一个点(x0,y0)来说，在这个点的导数定义是：</p> 
<p><img src="http://www.ml4ai.com/images/dx0.png" alt="image"></p> 
<p>$$ \text{在}x_0\text{处的导数定义} $$</p> 
<p>　　最原始的导数定义，如果这个不会，你就去翻上学时的内容，对于我们来说，不难看出，**导数就是该点的斜率；<br> $$ \text{如果我们要通过改变} x_0 \text{的值让}f(x0) \text{变小，我们必须把}x0\text{往导数反方向移动}$$<br> 根据定义，这个动作可以写这样的赋值：<br> $$x_0=x_0-step*f'(x_0)$$</p> 
<p>　　参数说明</p> 
<p>　　　　　　　　　step代表移动的步长系数，一般很小，不能移太大，因为移太大可能一步移过了最低点</p> 
<p>　　　　　　　　f'(x0)代表在x0处的导数</p> 
<p>　　如果$$f'(x_0)&gt;0$$，则往左移动$$step*|f'(x_0)|$$，右边竖线是导数的绝对值</p> 
<p>　　如果$$f'(x_0)&lt;0$$，则往右移动$$step*|f'(x_0)|$$，右边竖线是导数的绝对值</p> 
<p>　　经过不断的计算导数然后不断的这样移动后，我们的f(x0)就不断的变小，直到让它成0，无法再移动，这样f(x0)就达到了一个极小值。</p> 
<p>　　我们有了一个让f(x0)缩小的方法了，我们再来看看我们接下来重要的内容，我知道上面的可能对有人来说很基础，对有人来说却很困难，所以这个一定要确保能理解上面的方法，才能看下面的内容。</p> 
<p>　　我们来看看，在神经网络里，从通过类似上微小移动每个参数来改变$$loss$$。现在我们要讲方法，所以请务必把上面的东西看完并确保能完全理解，因为那是最基础的，所有后面的知识都基于上面的知识。</p> 
<p>　　我们找到了一个在连续曲线上找到较小值的方法，就是调整某个参数，按它的导数反向微小移动的方式。</p> 
<p>　　我们知道神经网络有权重还有偏执，我们这里用符号来表示，权重使用$$w_1,w_2,w_3,w_4,w_5,w_n$$,,,等来表示，偏执用$$b_1,b_2,b_3,b_4,b_5,b_n$$,,表示，我们这些都是网络中的参数，我们最终的目的是要<strong>调整这些参数的大小，让$$ loss $$最小。</strong></p> 
<p>　　我们知道神经网络是一个从输入层输入一个数据，然后通过计算流程，最终输出一数据，再与目标数据计算出一个<strong>loss</strong>，我们一定要清楚这个，特别是<strong>loss</strong>不是输出数据，而是计算结果与期望目标的欧氏距离。那么，对于一个确定的固定的输入神经网络，<strong>LOSS</strong>的大小取决于权重和偏执，调整其中一个权重，就意味着<strong>LOSS</strong>就会变化，那么同样的，我们根据导数的定义：调整其中一个权重$$w_n$$使权重改变一个很小的$$dw$$，调整以后$$loss$$就会改变，对应的$$loss$$有一个小的改变量，我们简写为$$dloss$$，那么根据导数的定义，我们可以得出：对于此刻的状态，损失对某个权重的导数可以表示为：</p> 
<p>$$\frac{dloss}{dw}$$</p> 
<p>　　这个解释一下，就是説移动一个参数后<strong>LOSS</strong>也会移动，两个的比值就是一个导数，也可当成斜率。</p> 
<p>　　这就是此刻状态<strong>loss</strong>对<strong>wn</strong>的导数，由于<strong>wn</strong>可以是任意一个权重，所以我们称以上式子为此刻<strong>loss</strong>对<strong>wn</strong>的偏导数<br> $$ \frac {\partial loss}{\partial w_n}$$<br> 这个是函数里的定义，所以不要问为什么这么取名字，原因是有多个自变量，每个变量的导数就是偏导数。同理，我们可以求出<strong>loss</strong>对任意的某个权重或某个偏执的偏导数，因为你只要把这个参数移动改一个微小量，对比<strong>loss</strong>的变化量就行。这样我们从理论上可以计算<strong>loss</strong>对任意参数的偏导数了。</p> 
<p>　　还记得我们的曲线找最小值的方法吗，没错，你只要把参数值往偏导数反方向移动一个微小的值，也等于减小<strong>loss</strong>。</p> 
<h1>3、训练调整参数</h1> 
<p>　　下面讲今天的主题，BP算法，也就是反向传播算法，它是干什么的，没错，它就是计算<strong>loss</strong>对每个参数的偏导数的，所以我们就来看看BP是怎么计算的，这里为了照顾入门的同学，我就不讲什么矩阵运算了，我们以例子来讲怎么计算偏导数。</p> 
<p>　　我们把loss的计算列成一个算式，在某次计算中：<strong>loss</strong> = .........，这里的省略号代表前面的计算流程，就是一堆数字和各种运算组合，由加法、乘法、平方、和Sigmoid函数组成，那么，我们如何求某个参数的偏导呢？我们就把这个参数设为变量x，不把它写成一个数字，比如把$$w_n$$写成自变量x，其它的所有参数都写成数字，于是我们就得到了一个只含有loss和x是变量的函数了，乘下的就是一元函数求导了，求loss对x的导数，求出导数以后，代入x值，就得到某参数的导数。</p> 
<p>　　我还是来一个简单的例子，假设一个简单的网络，loss展开后。</p> 
<pre><code class="language-math">loss = (1/(1+exp(-(w1*x1 + w2*x3 + w3 * x3)) - 1)
</code></pre> 
<pre><code>  请注意了，x1,w1,w3,w3,w4，等在一个计算中都是已知的，我们把除了w1的其它都写成已知的数字：
</code></pre> 
<pre><code class="language-math">loss = (1/(1+exp(-(w1*0.5+ 0.3*0.7 + 0.8*0.35)) - 1)
</code></pre> 
<p>　　我们发现，函数成了一个只含有loss和w1的函数了，然后一元函数求导：dloss/dw1，懂了吧？然后计算出的导数代入w1值，计算然出一个导数值，同理把式子写成只含有w2的式子，计算出w2的导数，这样就可以计算任意参数的导数了。</p> 
<p>　　这样我们可以通过固定其它参数，把要求导数的参数写成自变量，就能求参数的偏导数了。哈哈，这下知道了吧，求参数偏导数，那么反向传播是什么？其实是因为求的是一个复合函数，所以求导要一层一层的求，还能回忆函数的导数求法吗？从最外层往里层求，对应网络是从后面朝向前面求的，所以叫反向传播。</p> 
<p>　　计算量大，优化我不说了，在反向传播里，要规划计算的路径，以最小的计算量来完成，因为参数很多，某些网络有数亿参数，所以这个优化问题要在你充分理解自己来总结。</p> 
<p>　　这下我们只需要每次前向计算一次，再反向计算一次各个参数的偏导数，我们更新一次参数就行了，这个过程就是训练，不断的调整参数减小loss，这里要提一下这个计算量非常大，所以要使用像GPU这种高速的并行，并行的问题有空我再讲讲，这里不提，简单的说就是分工计算，把能分工的计算任务给不同的独立计算提供者，我们不断的训练让loss变小再次变小，也就是loss对每个权重和偏执的导数接近0，不能再小为止，这样loss很小的情况下，网络就成了我们预期的样子，可以用来对图像分类了。</p> 
<p>　　</p> 
<p>　　有问题也欢迎加入深度学习与人工智能交流群，一起交流学习讨论，QQ群号码 374685750。谢谢大家的支持，我有时间会发更多的文章，谢谢大家的支持。</p> 
<p>　　本文可以转载，但是必须声明出处链接和说明；</p></p>

	<hr />
	
		</div>
		<div id="push"></div>
    </div>
    
    <div id="footer">
      <div class="container">
        <p class="muted credit">&copy; 2018 | Power by <a href="http://www.ml4ai.com/">ML4AI</a> </p>
      </div>
    </div>
    
    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../js/jquery-1.11.1.min.js"></script>
    <script src="../js/bootstrap.min.js"></script>
    <script src="../js/prettify.js"></script>
    <audio autoplay="autoplay">
        <source src="/av/Alpha.mp3" type="audio/mpeg" />
        <source src="/av/breathAnd_Life.mp3" type="audio/mpeg" />
        <source src="/av/Victory.mp3" type="audio/mpeg" />
        您的浏览器不支持 audio 标签。
    </audio>
  </body>
</html>